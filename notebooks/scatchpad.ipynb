{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we'll import pytorch and check if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU only\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from FrEIA.modules import *\n",
    "from FrEIA.framework import *\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU available')\n",
    "else:\n",
    "    print('CPU only')\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_length = 41\n",
    "n_features = 1\n",
    "lstm_hidden = 100\n",
    "inn_hidden = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, define the path to the data we're using"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [],
   "source": [
    "if not use_cuda:\n",
    "    data_path = Path('C:\\\\Users\\\\dohert01\\\\PycharmProjects\\\\qPAI_cINN_uncertainty_estimation\\\\datasets')\n",
    "else:\n",
    "    data_path = Path('../datasets')\n",
    "\n",
    "experiment_name = \"FlowPhantom_insilico_complicated\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a look at the data. First of all, borrow some normalisation functions..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [],
   "source": [
    "def spectrum_normalisation(spectrum):\n",
    "    \"\"\"Applies z-score scaling to the initial pressure spectrum\"\"\"\n",
    "    mean = np.mean(spectrum)\n",
    "    std = np.std(spectrum)\n",
    "    norm = (spectrum - mean)/std\n",
    "    return norm\n",
    "\n",
    "def spectrum_processing(spectrum, allowed_datapoints):\n",
    "    \"\"\"Returns a normalised initial pressure spectrum with some of the values zeroed out\"\"\"\n",
    "    num_non_zero_datapoints = random.choice(allowed_datapoints)\n",
    "    a = np.zeros(len(spectrum))\n",
    "    a[:num_non_zero_datapoints] = 1\n",
    "    np.random.shuffle(a)\n",
    "\n",
    "    incomplete_spectrum = list(np.multiply(a, np.array(spectrum)))\n",
    "    non_zero_indices = np.nonzero(incomplete_spectrum)\n",
    "    non_zero_values = list(filter(None,incomplete_spectrum))\n",
    "    normalised_non_zero = spectrum_normalisation(non_zero_values)\n",
    "\n",
    "    i = 0\n",
    "    for index in non_zero_indices[0]:\n",
    "        incomplete_spectrum[index] = normalised_non_zero[i]\n",
    "        i+=1\n",
    "\n",
    "    normalised_incomplete_spectrum = np.array(incomplete_spectrum)\n",
    "\n",
    "    return normalised_incomplete_spectrum\n",
    "\n",
    "def batch_spectrum_processing(batch, allowed_datapoints):\n",
    "    processed = []\n",
    "\n",
    "    for spectrum in batch:\n",
    "\n",
    "        processed.append(spectrum_processing(spectrum, allowed_datapoints))\n",
    "    return torch.tensor(np.array(processed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's load the data from file"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "training_spectra_file = data_path / experiment_name / \"training_spectra.pt\"\n",
    "validation_spectra_file = data_path / experiment_name / \"validation_spectra.pt\"\n",
    "test_spectra_file = data_path / experiment_name / \"test_spectra.pt\"\n",
    "\n",
    "training_oxygenations_file = data_path / experiment_name / \"training_oxygenations.pt\"\n",
    "validation_oxygenations_file = data_path / experiment_name / \"validation_oxygenations.pt\"\n",
    "test_oxygenations_file = data_path / experiment_name / \"test_oxygenations.pt\"\n",
    "\n",
    "train_spectra_original = torch.load(training_spectra_file)\n",
    "train_oxygenations_original = torch.load(training_oxygenations_file)\n",
    "validation_spectra_original = torch.load(validation_spectra_file)\n",
    "validation_oxygenations_original = torch.load(validation_oxygenations_file)\n",
    "test_spectra_original = torch.load(test_spectra_file)\n",
    "test_oxygenations_original = torch.load(test_oxygenations_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's look at the dimensions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134624, 41])\n",
      "torch.Size([134624])\n",
      "tensor([634.9278, 600.2585, 600.2339, 587.4062, 580.4452, 573.9892, 582.9027,\n",
      "        597.7095, 601.8840, 641.6681, 655.6356, 704.5982, 730.0311, 739.1377,\n",
      "        762.7631, 768.2003, 789.5642, 808.6349, 811.7870, 835.5294, 866.5328,\n",
      "        886.8488, 918.7031, 905.1712, 913.7165, 913.7761, 913.4937, 919.7126,\n",
      "        915.4688, 919.2101, 887.4873, 870.8792, 905.5049, 883.7628, 876.9416,\n",
      "        888.4904, 881.3424, 888.5063, 892.4427, 879.3855, 869.1013],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(train_spectra_original.size())\n",
    "print(train_oxygenations_original.size())\n",
    "print(train_spectra_original[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "outputs": [],
   "source": [
    "# Zeroing out some of the spectrum data (randomly) and normalising\n",
    "allowed_datapoints = [10]\n",
    "\n",
    "train_spectra = batch_spectrum_processing(train_spectra_original, allowed_datapoints)\n",
    "validation_spectra = batch_spectrum_processing(validation_spectra_original, allowed_datapoints)\n",
    "test_spectra = batch_spectrum_processing(test_spectra_original, allowed_datapoints)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "outputs": [],
   "source": [
    "# Reshaping initial pressure spectra to fit LSTM input size\n",
    "train_spectra = torch.reshape(train_spectra, (len(train_spectra), len(train_spectra[0]), 1))\n",
    "validation_spectra = torch.reshape(validation_spectra, (len(validation_spectra), len(validation_spectra[0]), 1))\n",
    "test_spectra = torch.reshape(test_spectra, (len(test_spectra), len(test_spectra[0]), 1))\n",
    "\n",
    "train_oxygenations = torch.reshape(train_oxygenations_original,(len(train_oxygenations_original),1))\n",
    "validation_oxygenations = torch.reshape(validation_oxygenations_original,(len(validation_oxygenations_original),1))\n",
    "test_oxygenations = torch.tensor(np.float32(test_oxygenations_original))\n",
    "test_oxygenations = torch.reshape(test_oxygenations_original,(len(test_oxygenations_original),1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "outputs": [],
   "source": [
    "class MultiSpectralPressureO2Dataset(Dataset):\n",
    "    def __init__(self, spectra, oxygenations, transform=None, target_transform=None):\n",
    "        self.data = spectra\n",
    "        self.labels = oxygenations\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        # TODO - How can I add the input Gaussian to the dataloader? What does the input Gaussian look like? Would it be easier/preferable to input the outputs? Also, probably the Gaussian should be the same every time (?), such that the only information introduced is from the conditioning\n",
    "        # TODO - Alternativel, can have the Gaussian within the Wrapped Model class and it gets entered on the forward pass.\n",
    "        return data, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41])\n",
      "torch.Size([1])\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  1.3474,  0.0000,  0.0000, -0.0091,\n",
      "          0.0209,  0.0000,  1.2410,  0.0000,  0.0000,  0.0000,  1.3384,  0.0000,\n",
      "          0.0000,  0.2498,  0.0000,  0.0000, -0.5825,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000, -0.9523,  0.0000,  0.0000,  0.0000, -1.2184,  0.0000, -1.4351,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "tensor([0.2914])\n"
     ]
    }
   ],
   "source": [
    "def switch_seq_feat(tensor):\n",
    "    # Return a view of the tesor with axes rearranged\n",
    "    return torch.permute(tensor, (1, 0)).float()\n",
    "def float_transform(tensor):\n",
    "    return tensor.float()\n",
    "training_dataset = MultiSpectralPressureO2Dataset(\n",
    "    train_spectra, \n",
    "    train_oxygenations, \n",
    "    transform=switch_seq_feat, \n",
    "    target_transform=float_transform\n",
    ")\n",
    "training_dataloader = DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "data, label = next(iter(training_dataloader))\n",
    "\n",
    "print(data[0].size())\n",
    "print(label[0].size())\n",
    "print(data[0].type())\n",
    "print(label[0].type())\n",
    "print(data[0])\n",
    "print(label[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.Size([1, 41, 1])\n",
      "torch.FloatTensor\n",
      "LSTM output: torch.Size([1, 41, 100])\n",
      "Cond output: torch.Size([1, 4100, 1])\n"
     ]
    }
   ],
   "source": [
    "# Define the LSTM for the conditioning network\n",
    "# MASKING lAYER? Input of 2 with 2nd as binary to indicate if zero is real or not\n",
    "lstm = nn.LSTM(\n",
    "    input_size=n_features, # Input dimensions\n",
    "    hidden_size=lstm_hidden, # No. of neurons in gate networks\n",
    "    batch_first=False\n",
    ")\n",
    "class CondNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm =  lstm\n",
    "        #self.linear = nn.Linear(\n",
    "        #        in_features=lstm_hidden,\n",
    "        #        out_features=1\n",
    "        #)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.lstm(x)[0]\n",
    "        #out = self.linear(out)\n",
    "\n",
    "        return torch.reshape(out, (batch_size, -1, 1))\n",
    "\n",
    "# dummy inputs:\n",
    "x = torch.randn(batch_size, seq_length, n_features)\n",
    "print(data[0].type())\n",
    "print(x.size())\n",
    "print(x.type())\n",
    "\n",
    "y = lstm(x)\n",
    "print(f\"LSTM output: {y[0].shape}\")\n",
    "cond_network = CondNetwork()\n",
    "z = cond_network(x)\n",
    "print(f\"Cond output: {z.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Time to define the model... (both LSTM and INN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.9208, -1.2881,  0.4828, -1.1084,  0.9636,  0.7013, -2.8616,\n",
      "          -0.5026,  0.6625, -0.6185, -0.9619, -2.3585,  1.5150,  0.4384,\n",
      "           1.1502, -1.5456, -0.3578,  0.5617, -0.0088, -0.6101,  0.1700,\n",
      "          -0.3819,  0.6710,  0.1099,  1.7232,  0.1913,  0.0604, -2.1134,\n",
      "          -1.5997, -0.2156, -0.5071, -0.1564,  2.4289, -0.4607, -1.6133,\n",
      "          -1.1475, -0.1740, -2.3562,  1.4779,  1.2345,  0.1145, -0.2267,\n",
      "          -0.6371,  0.2498, -0.9373, -0.5317,  0.8264, -0.0949,  0.9595,\n",
      "          -0.7535,  0.6779, -0.3047,  0.3437,  0.6552, -1.7293,  1.9851,\n",
      "          -0.0731, -0.2930,  0.0750, -1.4737, -1.1889,  0.4880]]])\n",
      "torch.Size([1, 1, 40])\n"
     ]
    }
   ],
   "source": [
    "# Define the subnet for the invertible blocks (use the AllInOneBlack from FrEIA)\n",
    "n_blocks = 6  # No. of invertible blocks in INN\n",
    "def subnet(dims_in, dims_out):\n",
    "    return nn.Sequential(nn.Linear(dims_in, inn_hidden), nn.LeakyReLU(),\n",
    "                         nn.Linear(inn_hidden,  inn_hidden), nn.LeakyReLU(),\n",
    "                         nn.Linear(inn_hidden, dims_out))\n",
    "tens = torch.randn(1, 1, 62).normal_() # Is randn ok? Should it be seeded (probably for repeatability) or should it not be random at all?\n",
    "print(tens)\n",
    "out = subnet(62, 40).forward(tens)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next cell is defined as a SequentialINN..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4101x1 and 4101x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [323]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, seq_length_inn, n_features)\u001B[38;5;241m.\u001B[39mnormal_()\n\u001B[0;32m     10\u001B[0m cond \u001B[38;5;241m=\u001B[39m cond_network(torch\u001B[38;5;241m.\u001B[39mrandn(batch_size, seq_length, n_features))\n\u001B[1;32m---> 11\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[43minn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mcond\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\FrEIA\\framework\\sequence_inn.py:106\u001B[0m, in \u001B[0;36mSequenceINN.forward\u001B[1;34m(self, x_or_z, c, rev, jac)\u001B[0m\n\u001B[0;32m    104\u001B[0m         x_or_z, j \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_list[i](x_or_z, jac\u001B[38;5;241m=\u001B[39mjac, rev\u001B[38;5;241m=\u001B[39mrev)\n\u001B[0;32m    105\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 106\u001B[0m         x_or_z, j \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodule_list\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_or_z\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mc\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconditions\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mjac\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrev\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrev\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    108\u001B[0m     log_det_jac \u001B[38;5;241m=\u001B[39m j \u001B[38;5;241m+\u001B[39m log_det_jac\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x_or_z \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforce_tuple_output \u001B[38;5;28;01melse\u001B[39;00m x_or_z[\u001B[38;5;241m0\u001B[39m], log_det_jac\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\FrEIA\\modules\\all_in_one_block.py:248\u001B[0m, in \u001B[0;36mAllInOneBlock.forward\u001B[1;34m(self, x, c, rev, jac)\u001B[0m\n\u001B[0;32m    245\u001B[0m     x1c \u001B[38;5;241m=\u001B[39m x1\n\u001B[0;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m rev:\n\u001B[1;32m--> 248\u001B[0m     a1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx1c\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    249\u001B[0m     x2, j2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_affine(x2, a1)\n\u001B[0;32m    250\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\qpai-cinn-uncertainty-estimation-dfL09TuT-py3.8\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (4101x1 and 4101x128)"
     ]
    }
   ],
   "source": [
    "seq_length_inn = 2\n",
    "input_dims = (2,)\n",
    "inn = SequenceINN(*input_dims)\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    # TODO - Consider clamping and permute_soft arguments\n",
    "    inn.append(AllInOneBlock, cond=0, cond_shape=(4100, ), subnet_constructor=subnet)\n",
    "\n",
    "input = torch.randn(batch_size, seq_length_inn, n_features).normal_()\n",
    "cond = cond_network(torch.randn(batch_size, seq_length, n_features))\n",
    "z = inn(input, c=[cond])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, cond_network, inn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cond_network = cond_network\n",
    "        self.inn = inn\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        cond = self.cond_network(x)\n",
    "        #cond = torch.permute(cond, (0,2,1))\n",
    "        norm = torch.randn(batch_size, 41, 1).normal_()\n",
    "        z = self.inn(norm, [cond])\n",
    "        zz = sum(torch.sum(o**2, dim=1) for o in z)\n",
    "        jac = self.inn.jacobian(run_forward=False)\n",
    "\n",
    "        return zz, jac\n",
    "\n",
    "    def reverse_sample(self, z, cond):\n",
    "        return self.inn(z, cond, rev=True)\n",
    "\n",
    "model = WrappedModel(cond_network, inn)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# Time to define the training loop...\n",
    "# Defining optimizer etc\n",
    "n_epochs = 10\n",
    "decay_by = 0.01\n",
    "weight_decay = 1e-5\n",
    "betas = (0.9, 0.999)\n",
    "log10_lr = -4.0  # Log learning rate\n",
    "lr = 10**log10_lr\n",
    "params_trainable = (list(filter(lambda p: p.requires_grad, model.inn.parameters()))\n",
    "                  + list(model.cond_network.parameters()))\n",
    "gamma = decay_by**(1./n_epochs)\n",
    "optim = torch.optim.Adam(params_trainable, lr=lr, betas=betas, eps=1e-6, weight_decay=weight_decay)\n",
    "weight_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=gamma)\n",
    "\n",
    "\n",
    "# a very basic training loop\n",
    "for data, label in training_dataloader:\n",
    "    \"\"\"\n",
    "    iterator = tqdm.tqdm(\n",
    "        iter(training_dataloader),\n",
    "        total=len(training_dataloader),\n",
    "        leave=False,\n",
    "        mininterval=1.,\n",
    "        ncols=83\n",
    "    )\n",
    "    for x in iterator:\n",
    "    \"\"\"\n",
    "    #print(len(x))\n",
    "    #print(x[0].size())\n",
    "    #break\n",
    "    optim.zero_grad()\n",
    "    # Send data to GPU (https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)\n",
    "    # data, label = data.to(device), label.to(device)\n",
    "    # pass to INN and get transformed variable z and log Jacobian determinant\n",
    "    zz, jac = model(data)\n",
    "    # calculate the negative log-likelihood of the model with a standard normal prior\n",
    "    neg_log_likeli = 0.5 * zz - jac\n",
    "    loss = torch.mean(neg_log_likeli) / 40  # tot_output_size\n",
    "    # backpropagate and update the weights\n",
    "    loss.backward()\n",
    "    optim.step()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inn = SequenceINN(41, 1)\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    # TODO - Consider clamping and permute_soft arguments\n",
    "    inn.append(AllInOneBlock, cond=0, cond_shape=(100, 1), subnet_constructor=subnet)\n",
    "    \n",
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, cond_network, inn):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cond_network = cond_network\n",
    "        self.inn = inn\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #cond = [x, self.cond_network(x).squeeze()]\n",
    "        cond = self.cond_network(x)\n",
    "\n",
    "        z = self.inn(x, cond)\n",
    "        zz = sum(torch.sum(o**2, dim=1) for o in z)\n",
    "        jac = self.inn.jacobian(run_forward=False)\n",
    "\n",
    "        return zz, jac\n",
    "\n",
    "    def reverse_sample(self, z, cond):\n",
    "        return self.inn(z, cond, rev=True)\n",
    "\n",
    "model = WrappedModel(lstm, inn)\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Time to define the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Defining optimizer etc\n",
    "n_epochs = 10\n",
    "decay_by = 0.01\n",
    "weight_decay = 1e-5\n",
    "betas = (0.9, 0.999)\n",
    "log10_lr = -4.0                     # Log learning rate\n",
    "lr = 10**log10_lr\n",
    "lr_feature_net = lr                 # lr of the cond. network\n",
    "params_trainable = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "gamma = decay_by**(1./n_epochs)\n",
    "optim = torch.optim.Adam(params_trainable, lr=lr, betas=betas, eps=1e-6, weight_decay=weight_decay)\n",
    "weight_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=1, gamma=gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# a very basic training loop\n",
    "#for data, label in training_dataloader:\n",
    "iterator = tqdm.tqdm(enumerate(iter(training_dataloader)),\n",
    "                     total=len(training_dataloader),\n",
    "                     leave=False,\n",
    "                     mininterval=1.,\n",
    "                     ncols=83)\n",
    "for i_batch, x in iterator:\n",
    "    #print(len(x))\n",
    "    #print(x[0].size())\n",
    "    #break\n",
    "    optim.zero_grad()\n",
    "    # Send data to GPU (https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)\n",
    "    #data, label = data.to(device), label.to(device)\n",
    "    # pass to INN and get transformed variable z and log Jacobian determinant\n",
    "    zz, jac = model.forward(x[0])\n",
    "    # calculate the negative log-likelihood of the model with a standard normal prior\n",
    "    neg_log_likeli = 0.5 * zz - jac\n",
    "    loss = torch.mean(neg_log_likeli) / 41#tot_output_size\n",
    "    # backpropagate and update the weights\n",
    "    loss.backward()\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}